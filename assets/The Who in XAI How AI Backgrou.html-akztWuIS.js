import{_ as i,r as l,o,c as n,a as e,b as a,d as r,e as s}from"./app-v3FIsn7w.js";const h={},c=e("h1",{id:"从xai到用户背景-人工智能背景如何塑造对ai解释的感知",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#从xai到用户背景-人工智能背景如何塑造对ai解释的感知"},[e("span",null,"从XAI到用户背景：人工智能背景如何塑造对AI解释的感知")])],-1),p=e("iframe",{width:"720",height:"400",src:"https://www.youtube.com/embed/PdTZPV_nVhM?si=A4IMURebFUKGuaL5",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:""},null,-1),d=s('<h2 id="论文总结" tabindex="-1"><a class="header-anchor" href="#论文总结"><span>论文总结</span></a></h2><h3 id="研究机构" tabindex="-1"><a class="header-anchor" href="#研究机构"><span>研究机构</span></a></h3><p>佐治亚理工学院，微软，Illumio，佐治亚理工学院，IBM 研究院</p><h3 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h3><p>本研究由Upol Ehsan、Samir Passi、Vera Liao和Larry Chan等人进行，他们来自不同的学术和工业背景。他们通过一项用户研究探讨了人工智能（AI）背景如何影响人们对AI解释的感知。研究发现，AI背景（或缺乏 thereof）对用户的理解、信任以及对AI系统行为的反应有着显著的影响。特别是，具有AI背景的用户倾向于更重视系统的“智能”表达，而非仅仅是结果，而没有AI背景的用户可能更多地基于确认和稳定性来评价AI的行为。研究通过实证数据提出，设计人员在开发XAI（可解释的人工智能）时应考虑到用户的AI背景差异，并建议采用适应不同背景的设计策略，以避免不适当的信任或对系统的误解。</p><h3 id="问题发现" tabindex="-1"><a class="header-anchor" href="#问题发现"><span>问题发现</span></a></h3><p>作者们观察到，在人工智能解释的接收和理解上，用户之间的个体差异是显著的。具体来说，他们发现：</p><ol><li>AI背景：拥有AI课程背景的人与无此背景的人在评价AI系统时表现出不同的偏好。</li><li>对解释的依赖：用户可能基于对系统的信任程度、错误容忍度以及对AI智能的感知来评估解释的价值。</li><li>数字和算法的盲目信任：无论是AI背景还是非AI背景的用户，都倾向于将数字或算法的存在视为AI智能的象征。</li></ol><h3 id="解决方案" tabindex="-1"><a class="header-anchor" href="#解决方案"><span>解决方案</span></a></h3><p>为了缓解因AI背景差异导致的解释理解问题，研究提出以下建议：</p><ol><li>多元化解释方式：设计既包含功能性解释（如AD机器人）又具有可解释性（如RG机器人）的混合风格解释。</li><li>增加反思元素：通过将数字与语言结合，促使用户进行更深入的理解和思考（例如提供可以“展开”的Q值以解释自然语言）。</li><li>教育改革：针对AI教育，强调伦理和技术的融合，培养未来设计者理解和考虑不同用户背景的能力。</li></ol><h3 id="结果" tabindex="-1"><a class="header-anchor" href="#结果"><span>结果</span></a></h3><p>研究结果显示：</p><ol><li>用户评价：AI背景的参与者更倾向于根据系统的“智能”来评价，而非仅仅是行动的结果或确认性反馈。</li><li>信任与理解：非AI背景的用户在寻求稳定性和确认时，对无解释性（NR）机器人的接受度较高，而有解释性的RG机器人则更受青睐。</li><li>数字的信任偏误：尽管存在数字和算法的盲目信任，但通过设计可以引导用户进行更有意识的反思。</li></ol><h3 id="结论" tabindex="-1"><a class="header-anchor" href="#结论"><span>结论</span></a></h3><p>研究强调了在开发XAI时考虑用户背景的重要性，并提出了针对性的设计策略。这有助于减少潜在的解释性陷阱（EPs），即无意识地通过添加AI解释而产生的负面后果。</p><h2 id="举一反三" tabindex="-1"><a class="header-anchor" href="#举一反三"><span>举一反三</span></a></h2><h4 id="q1-用户对ai背景的理解如何影响他们对ai解释的接受度" tabindex="-1"><a class="header-anchor" href="#q1-用户对ai背景的理解如何影响他们对ai解释的接受度"><span>Q1：用户对AI背景的理解如何影响他们对AI解释的接受度？</span></a></h4><p>A1：用户的AI背景，如技术水平和教育程度，会影响他们解读AI系统提供的解释。具有AI背景的用户可能更倾向于理解和接纳复杂、技术性的解释，而非AI背景的用户则可能更偏好简单且能确认系统行为的解释。</p><h4 id="q2-不同类型的ai解释-自然语言、数字和行动声明-如何影响用户的感知" tabindex="-1"><a class="header-anchor" href="#q2-不同类型的ai解释-自然语言、数字和行动声明-如何影响用户的感知"><span>Q2：不同类型的AI解释（自然语言、数字和行动声明）如何影响用户的感知？</span></a></h4><p>A2：实验结果显示，提供理由（rationale）的AI解释被看作最具说服力，其次是非解释性的行动声明（action-declaring），而仅给出数字的解释则在确认系统性能方面受到非AI背景用户的价值肯定。</p><h4 id="q3-如何设计ai解释以适应不同背景的用户" tabindex="-1"><a class="header-anchor" href="#q3-如何设计ai解释以适应不同背景的用户"><span>Q3：如何设计AI解释以适应不同背景的用户？</span></a></h4><p>A3：为了弥合创造者和消费者之间的差距，可以采用混合式方法。例如，结合功能性的自然语言解释与提供机制理解的数值（如Q值），使用户能根据需要深入了解系统决策的原因。同时，对于非AI背景用户，强调稳定性和可预测性可能是更有效的设计策略。</p><hr>',24),A={href:"https://dl.acm.org/doi/fullHtml/10.1145/3613904.3642474",target:"_blank",rel:"noopener noreferrer"},I={href:"https://www.mix-copilot.com/",target:"_blank",rel:"noopener noreferrer"};function u(m,f){const t=l("ExternalLinkIcon");return o(),n("div",null,[c,p,d,e("p",null,[a("原文地址："),e("a",A,[a("https://dl.acm.org/doi/fullHtml/10.1145/3613904.3642474"),r(t)])]),e("p",null,[a("内容由"),e("a",I,[a("MiX Copilot"),r(t)]),a("基于大语言模型生成，有可能存在错误的风险。")])])}const _=i(h,[["render",u],["__file","The Who in XAI How AI Backgrou.html.vue"]]),b=JSON.parse('{"path":"/posts/papers/AI/The%20Who%20in%20XAI%20How%20AI%20Backgrou.html","title":"从XAI到用户背景：人工智能背景如何塑造对AI解释的感知","lang":"en-US","frontmatter":{"description":"本文通过实验探索了AI背景如何影响用户对解释的感知，发现AI背景会影响用户对解释的期望和理解，进而影响他们对AI系统的信任和互动。","date":"2024/5/15 21:00:45","head":[["meta",{"property":"og:url","content":"https://www.xuezhirong.com/posts/papers/AI/The%20Who%20in%20XAI%20How%20AI%20Backgrou.html"}],["meta",{"property":"og:site_name","content":"薛志荣"}],["meta",{"property":"og:title","content":"从XAI到用户背景：人工智能背景如何塑造对AI解释的感知"}],["meta",{"property":"og:description","content":"本文通过实验探索了AI背景如何影响用户对解释的感知，发现AI背景会影响用户对解释的期望和理解，进而影响他们对AI系统的信任和互动。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"article:published_time","content":"2024-05-15T13:00:45.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"从XAI到用户背景：人工智能背景如何塑造对AI解释的感知\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-05-15T13:00:45.000Z\\",\\"dateModified\\":null,\\"author\\":[]}"]]},"headers":[{"level":2,"title":"论文总结","slug":"论文总结","link":"#论文总结","children":[{"level":3,"title":"研究机构","slug":"研究机构","link":"#研究机构","children":[]},{"level":3,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":3,"title":"问题发现","slug":"问题发现","link":"#问题发现","children":[]},{"level":3,"title":"解决方案","slug":"解决方案","link":"#解决方案","children":[]},{"level":3,"title":"结果","slug":"结果","link":"#结果","children":[]},{"level":3,"title":"结论","slug":"结论","link":"#结论","children":[]}]},{"level":2,"title":"举一反三","slug":"举一反三","link":"#举一反三","children":[]}],"git":{"updatedTime":null,"contributors":[]},"filePathRelative":"posts/papers/AI/The Who in XAI How AI Backgrou.md","excerpt":"\\n<iframe width=\\"720\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/PdTZPV_nVhM?si=A4IMURebFUKGuaL5\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen=\\"\\"></iframe>"}');export{_ as comp,b as data};
